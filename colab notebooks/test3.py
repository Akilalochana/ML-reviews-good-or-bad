# -*- coding: utf-8 -*-
"""test3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V4fluUvMqDMWJqMHMItkMyAdRIhrS8TK

classification problem
"""

pip install numpy pandas matplotlib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv("/content/sentiment_analysis.csv")
data.head()

"""0-positive   1-negative"""

data.info()

"""preprocessing"""

data.shape

data.duplicated().sum()

data.isnull().sum()

#---------------text preprocessing------
# work with text data not numarical
# 1.convert all lettersto lowercase
# remove links
# remove punctuation marks
# remove numbers
# remove stop WORDS
# stemming - creating/creative all convet to => create

import re
import string

"""convert to lowecase"""

data["tweet"].head(5)

data["tweet"] = data["tweet"].apply(lambda x: " ".join(x.lower() for x in x.split()))

data["tweet"].head(5)

# remove links

data["tweet"] = data['tweet'].apply(lambda x: " ".join(re.sub(r'^https?:\/\/.*[\r\n]*', '', x, flags=re.MULTILINE) for x in x.split()))

data["tweet"].head(5)

string.punctuation

def remove_punctuations(text):
    for punctuation in string.punctuation:
        text = text.replace(punctuation, '')
    return text

data["tweet"] = data["tweet"].apply(remove_punctuations)

data["tweet"].tail(5)

data["tweet"] = data['tweet'].str.replace(r'\d+', '', regex=True)

data["tweet"].tail(5)

!pip install nltk

import nltk

nltk.download('stopwords')  # âœ… No need to specify download_dir

from nltk.corpus import stopwords

# Load English stopwords
stop_words = set(stopwords.words('english'))
print(stop_words)

data["tweet"].head(5)

data["tweet"] = data["tweet"].apply(lambda x: " ".join(x for x in x.split() if x not in stop_words))

data["tweet"].head(5)

# get only base word
from nltk.stem import PorterStemmer
ps = PorterStemmer()

data["tweet"] = data["tweet"].apply(lambda x: " ".join(ps.stem(x) for x in x.split()))

data["tweet"].head(5)

data

# convert numaric
from collections import Counter
vocab = Counter()

# vocab.update(['apple','banana','mango'])

# vocab

for sentence in data['tweet']:
  vocab.update(sentence.split())

vocab

len(vocab)

data.shape

tokens = [key for key in vocab if vocab[key] > 15]

tokens

len(tokens)

def save_vocabulary(lines, filename):
    data = '\n'.join(lines)
    file = open(filename, 'w', encoding="utf-8")
    file.write(data)
    file.close()

save_vocabulary(tokens, '/content/vocabulary.txt')

"""devide data to teat and train"""

X = data['tweet']
y = data['label']

y

!pip install scikit-learn

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

X_train.shape

X_test.shape

#vectorazation
def vectorizer(ds, vocabulary):
    vectorized_lst = []

    for sentence in ds:
        sentence_lst = np.zeros(len(vocabulary))

        for i in range(len(vocabulary)):
            if vocabulary[i] in sentence.split():
                sentence_lst[i] = 1

        vectorized_lst.append(sentence_lst)

    vectorized_lst_new = np.asarray(vectorized_lst, dtype=np.float32)

    return vectorized_lst_new

vectorized_x_train = vectorizer(X_train, tokens)

vectorized_x_test = vectorizer(X_test, tokens)

vectorized_x_train

y_train

vectorized_x_test

y_test

y_train.value_counts()

plt.pie(np.array([y_train.value_counts()[0], y_train.value_counts()[1]]), labels=['Positive', 'Negative'])
plt.show()

#    handel imbelace data

!pip install imbalanced-learn

from imblearn.over_sampling import SMOTE
smote = SMOTE()
vectorized_x_train_smote, y_train_smote = smote.fit_resample(vectorized_x_train, y_train)
print(vectorized_x_train_smote.shape, y_train_smote.shape)

y_train_smote.value_counts()

plt.pie(np.array([y_train_smote.value_counts()[0], y_train_smote.value_counts()[1]]), labels=['Positive', 'Negative'])
plt.show()

vectorized_x_train_smote

y_train_smote

vectorized_x_test

y_test

"""MODEL TRANING-========================"""

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

def training_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    pr = round(precision_score(y_act, y_pred), 3)
    rec = round(recall_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Training Scores:\n\tAccuracy = {acc}\n\tPrecision = {pr}\n\tRecall = {rec}\n\tF1-Score = {f1}')

def validation_scores(y_act, y_pred):
    acc = round(accuracy_score(y_act, y_pred), 3)
    pr = round(precision_score(y_act, y_pred), 3)
    rec = round(recall_score(y_act, y_pred), 3)
    f1 = round(f1_score(y_act, y_pred), 3)
    print(f'Testing Scores:\n\tAccuracy = {acc}\n\tPrecision = {pr}\n\tRecall = {rec}\n\tF1-Score = {f1}')

#logictic regression

lr = LogisticRegression()
lr.fit(vectorized_x_train_smote, y_train_smote)

y_train_pred = lr.predict(vectorized_x_train_smote)

y_train_pred

y_train_smote

training_scores(y_train_smote, y_train_pred)

y_test_pred = lr.predict(vectorized_x_test)

y_test_pred

validation_scores(y_test, y_test_pred)

"""# NB"""

mnb = MultinomialNB()
mnb.fit(vectorized_x_train_smote, y_train_smote)

y_train_pred = mnb.predict(vectorized_x_train_smote)

y_test_pred = mnb.predict(vectorized_x_test)

training_scores(y_train_smote, y_train_pred)

validation_scores(y_test, y_test_pred)

dt = DecisionTreeClassifier()

dt.fit(vectorized_x_train_smote, y_train_smote)

y_train_pred = dt.predict(vectorized_x_train_smote)

y_test_pred = dt.predict(vectorized_x_test)

training_scores(y_train_smote, y_train_pred)

validation_scores(y_test, y_test_pred)

rf = RandomForestClassifier()

rf.fit(vectorized_x_train_smote, y_train_smote)

y_train_pred = rf.predict(vectorized_x_train_smote)

y_test_pred = rf.predict(vectorized_x_test)

training_scores(y_train_smote, y_train_pred)

validation_scores(y_test, y_test_pred)

svm = SVC()

svm.fit(vectorized_x_train_smote, y_train_smote)

y_train_pred = svm.predict(vectorized_x_train_smote)

y_test_pred = svm.predict(vectorized_x_test)

training_scores(y_train_smote, y_train_pred)

validation_scores(y_test, y_test_pred)

# save logictic model
import pickle
with open('/content/model.pickle','wb') as file:
  pickle.dump(lr,file)

